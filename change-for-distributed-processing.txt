# Distributed Processing Implementation Plan for Document OCR/LLM System

## Overview
This document outlines the comprehensive plan to extend the existing document OCR system with distributed LLM processing capabilities, including Kafka message queues, MongoDB storage, and document history management.

## Current Architecture
- Frontend: React + Vite application
- Backend: FastAPI service with PaddleOCR
- Infrastructure: Docker Compose setup

## Target Architecture
- Frontend: Enhanced React UI with LLM options and history management
- doc-ocr-backend: Central orchestrator for file processing and job management
- doc-ocr-processing: New distributed service for LLM processing
- Infrastructure: Kafka, Zookeeper, MongoDB, existing services

## Processing Flow

### Phase 1: File Upload & Image Distribution
1. Frontend uploads file + selects LLM model â†’ doc-ocr-backend
2. doc-ocr-backend splits document into page images â†’ publishes to Kafka
3. doc-ocr-backend creates job record in MongoDB with status "processing"

### Phase 2: Individual Page Processing
4. doc-ocr-processing consumes images from Kafka â†’ processes via Vertex AI
5. doc-ocr-processing stores individual page results in MongoDB

### Phase 3: Job Completion Detection & Aggregation Trigger
6. doc-ocr-backend polls MongoDB every 5 seconds to check job completion status
7. doc-ocr-backend when all pages processed â†’ publishes "aggregation signal" to Kafka

### Phase 4: Document Aggregation
8. doc-ocr-processing consumes aggregation signal from Kafka
9. doc-ocr-processing retrieves all page results from MongoDB
10. doc-ocr-processing calls Vertex AI for summary + markdown generation
11. doc-ocr-processing stores final results in MongoDB

### Phase 5: Results Display
12. Frontend polls doc-ocr-backend every 5 seconds for job status
13. doc-ocr-backend queries MongoDB â†’ returns status to frontend
14. Frontend displays results when complete

## Kafka Topics

### page-processing-topic
Purpose: Individual page images for processing
Message Format:
```json
{
  "job_id": "uuid",
  "page_number": 1,
  "image_data": "base64_encoded_image",
  "llm_model": "gemini-2.0-flash"
}
```

### aggregation-trigger-topic  
Purpose: Job completion signals for document aggregation
Message Format:
```json
{
  "job_id": "uuid",
  "llm_model": "gemini-2.0-flash",
  "total_pages": 5
}
```

## MongoDB Schemas

### jobs_collection
```json
{
  "job_id": "uuid",
  "file_name": "invoice.pdf",
  "file_type": "pdf",
  "total_pages": 5,
  "processed_pages": 3,
  "llm_model": "gemini-2.0-flash",
  "processing_type": "llm",
  "status": "processing",
  "created_at": "2024-01-15T14:30:00Z",
  "updated_at": "2024-01-15T14:32:00Z",
  "completed_at": null,
  "analysis_duration_seconds": null
}
```

### page_results_collection
```json
{
  "job_id": "uuid",
  "page_number": 1,
  "extracted_text": "Invoice content...",
  "confidence_score": 0.95,
  "status": "completed",
  "created_at": "2024-01-15T14:31:00Z"
}
```

### final_results_collection
```json
{
  "job_id": "uuid",
  "document_overview": "This is an invoice for...",
  "markdown_content": "# Invoice Analysis\n\n## Page 1\n...",
  "status": "completed",
  "created_at": "2024-01-15T14:35:00Z"
}
```

## Component Responsibilities

### doc-ocr-backend (Enhanced)
- File upload handling
- Document splitting into page images
- Kafka producer for page images
- MongoDB job tracking and status monitoring
- Job completion detection via MongoDB polling (5-second intervals)
- Aggregation signal publishing to Kafka
- Frontend API endpoints for status queries
- Document history API endpoints
- Bulk deletion API endpoint

### doc-ocr-processing (New Service)
- Kafka consumer for page images
- Individual page Vertex AI processing
- Page results storage in MongoDB
- Kafka consumer for aggregation signals
- Final document aggregation (summary + markdown)
- Final results storage in MongoDB

### Frontend (Enhanced)
- Updated UI with OCR/LLM processing options
- LLM model dropdown (Gemini-2.0-Flash, Gemini-2.5-Pro)
- Status polling for job completion
- Results display component
- Document history table with analysis time
- Download functionality for markdown results
- Delete All functionality

## UI Layout Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OnPrem Document AI Demo                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [File Upload Section]                  â”‚
â”‚  [OCR Button] [LLM Button] [Model â–¼]    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [Current Document Results Display]     â”‚
â”‚  â€¢ AI Overview                          â”‚
â”‚  â€¢ Markdown Content                     â”‚
â”‚  â€¢ Download Button                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“‹ Analysis History        [Delete All]â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Document  â”‚Started â”‚Typeâ”‚Pagesâ”‚Statusâ”‚ â”‚
â”‚  â”‚ Name      â”‚At      â”‚    â”‚     â”‚&Time â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ğŸ“„invoice  â”‚Jan 15  â”‚PDF â”‚ 5   â”‚âœ… 2m34sâ”‚ â”‚
â”‚  â”‚.pdf       â”‚2:30 PM â”‚    â”‚     â”‚LLM   â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ğŸ“„contract â”‚Jan 15  â”‚Img â”‚ 1   â”‚ğŸ”„ --  â”‚ â”‚
â”‚  â”‚.png       â”‚1:15 PM â”‚    â”‚     â”‚OCR   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## API Endpoints

### New Endpoints (doc-ocr-backend)
- `POST /api/llm/process/` - LLM processing initiation
- `GET /api/documents/history` - Document history retrieval
- `GET /api/documents/{job_id}/results` - Specific document results
- `GET /api/documents/{job_id}/status` - Job status polling
- `DELETE /api/documents/all` - Bulk deletion

### Enhanced Endpoints
- `POST /api/ocr/process/` - Enhanced with job tracking

## Docker Services

### docker-compose.yml additions
```yaml
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-kafka:latest
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092

  mongodb:
    image: mongo:latest
    ports:
      - "27017:27017"

  doc-ocr-processing:
    build: ./doc-ocr-processing
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      MONGODB_URL: mongodb://mongodb:27017
      VERTEX_AI_PROJECT_ID: ${GCP_PROJECT_ID}
```

## Vertex AI Integration

### Model Endpoints
```javascript
const modelEndpoints = {
  'gemini-2.0-flash': 'projects/{PROJECT_ID}/locations/{LOCATION}/publishers/google/models/gemini-2.0-flash-exp',
  'gemini-2.5-pro': 'projects/{PROJECT_ID}/locations/{LOCATION}/publishers/google/models/gemini-2.5-pro'
};
```

### Processing Types
1. Individual Page Processing: Image â†’ Text extraction
2. Document Aggregation: All page texts â†’ Summary + Markdown formatting

## Implementation Timeline

### Phase 1: Infrastructure Setup
- MongoDB, Kafka, Zookeeper services
- Basic doc-ocr-processing service
- Enhanced doc-ocr-backend with MongoDB

### Phase 2: Core Processing
- Kafka producers/consumers
- Vertex AI integration
- Job tracking and completion detection

### Phase 3: Frontend Enhancement
- LLM processing UI
- History management
- Results display

### Phase 4: Advanced Features
- Analysis time tracking
- Download functionality
- Bulk operations

## Key Benefits
- Distributed processing for scalability
- Persistent job tracking and history
- Multiple AI model options
- Comprehensive results management
- Performance analytics
- User-friendly interface

## Technical Considerations
- Error handling across distributed components
- Message queue reliability and ordering
- Database consistency and transactions
- Frontend polling optimization (5-second intervals)
- Resource management for large documents
- Security for GCP credentials and API access

## Demo Implementation Details (MISSING - NEEDS IMPLEMENTATION)

### Critical Missing Components
1. **GCP Vertex AI Integration Code**
   - Vertex AI client setup with authentication
   - Prompt templates for page processing and document aggregation
   - Model endpoint configuration

2. **MongoDB Connection Setup**
   - Database client initialization
   - Collection schemas and indexing
   - Connection string configuration

3. **Environment Configuration**
   - `.env` files with GCP project settings
   - Kafka connection parameters
   - MongoDB connection strings

4. **Basic Error Handling**
   - Vertex AI service unavailable fallbacks
   - Simple user-friendly error messages
   - Kafka connection retry logic

### Implementation Priorities
1. **Phase 1**: Infrastructure setup (MongoDB, Kafka containers)
2. **Phase 2**: Basic Vertex AI integration
3. **Phase 3**: Job tracking with 5-second polling
4. **Phase 4**: Frontend integration and status updates